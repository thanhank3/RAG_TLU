import os
import pandas as pd
from collections import defaultdict
from sentence_transformers import SentenceTransformer
import chromadb
from unstructured.partition.auto import partition
from unstructured.chunking.basic import chunk_elements

# ==== C·∫•u h√¨nh ====
input_dir = r"D:\DATN\input_files"
output_dir = r"D:\DATN\Output2"
os.makedirs(output_dir, exist_ok=True)

# ==== Kh·ªüi t·∫°o ChromaDB ====
client = chromadb.PersistentClient(path=os.path.join(output_dir, "chroma_db"))
collection = client.get_or_create_collection(name="rag_collection")

# ==== N·∫øu ƒë√£ c√≥ d·ªØ li·ªáu ‚Üí kh√¥ng l√†m l·∫°i ====
if collection.count() > 0:
    print(f"‚úÖ Chroma ƒë√£ c√≥ {collection.count()} ƒëo·∫°n. Kh√¥ng c·∫ßn x·ª≠ l√Ω l·∫°i.")
    exit()

# ==== T·∫£i model SentenceTransformer ====
model = SentenceTransformer("BAAI/bge-m3")

# ==== Ingest d·ªØ li·ªáu ====
file_count = 0
for filename in os.listdir(input_dir):
    file_path = os.path.join(input_dir, filename)
    if not os.path.isfile(file_path):
        continue

    print(f"üìÑ ƒêang x·ª≠ l√Ω file: {filename}")
    try:
        chunk_texts = []
        # metadatas = []

        if filename.lower().endswith((".xlsx", ".xls")):
            xlsx = pd.read_excel(file_path, sheet_name=None)
            for sheet_name, df in xlsx.items():
                df = df.fillna("").astype(str)
                df.columns = df.columns.str.strip().str.lower()

                # === File ƒëi·ªÉm sinh vi√™n ===
                if "m√£ sinh vi√™n" in df.columns and "t√™n h·ªçc ph·∫ßn" in df.columns:
                    grouped_by_mssv = defaultdict(list)

                    for _, row in df.iterrows():
                        mssv = row.get("m√£ sinh vi√™n", "")
                        grouped_by_mssv[mssv].append(row)

                    for mssv, rows in grouped_by_mssv.items():
                        # L·ªçc m√¥n tr√πng ‚Üí gi·ªØ ƒëi·ªÉm cao h∆°n
                        unique_courses = {}
                        for row in rows:
                            ma_hp = row.get("m√£ h·ªçc ph·∫ßn", "").strip()
                            diem_chu = row.get("ƒëi·ªÉm ch·ªØ", "").strip().upper()
                            diem_thap = {"A": 4, "B": 3, "C": 2, "D": 1, "F": 0}.get(diem_chu, 0)
                            if ma_hp not in unique_courses or diem_thap > unique_courses[ma_hp]['diem']:
                                unique_courses[ma_hp] = {
                                    'row': row,
                                    'diem': diem_thap
                                }

                        # T√≠nh t·ªïng TC v√† t·ªïng TC √ó ƒëi·ªÉm
                        tong_tc = 0
                        tong_tc_diem = 0
                        mon_list = []
                        for info in unique_courses.values():
                            row = info['row']
                            diem_chu = row.get("ƒëi·ªÉm ch·ªØ", "").strip().upper()
                            if diem_chu == "F":
                                continue
                            ma_hp = row.get("m√£ h·ªçc ph·∫ßn", "").upper()
                            ten = row.get("t√™n h·ªçc ph·∫ßn", "").lower()
                            try:
                                tc = float(row.get("s·ªë tc", "0"))
                            except:
                                continue
                            # Lo·∫°i Ti·∫øng Anh tƒÉng c∆∞·ªùng, Th·ªÉ ch·∫•t 1 TC
                            if "tƒÉng c∆∞·ªùng" in ten or ma_hp.startswith("TATC"):
                                continue
                            if tc == 1 and any(kw in ten for kw in ["b√≥ng chuy·ªÅn", "c·∫ßu l√¥ng", "c·ªù vua", "qu·∫ßn v·ª£t"]):
                                continue
                            diem_thap = {"A": 4.0, "B": 3.0, "C": 2.0, "D": 1.0}.get(diem_chu, 0)
                            tong_tc += tc
                            tong_tc_diem += tc * diem_thap
                            mon_list.append(row)

                        gpa = round(tong_tc_diem / tong_tc, 2) if tong_tc > 0 else 0

                        joined = f"[D·ªØ li·ªáu ƒëi·ªÉm to√†n kh√≥a]\nM√£ sinh vi√™n: {mssv}\nT·ªïng s·ªë t√≠n ch·ªâ t√≠ch l≈©y: {tong_tc}\nT·ªïng ƒëi·ªÉm x t√≠n ch·ªâ: {tong_tc_diem:.2f}\nGPA to√†n kh√≥a: {gpa}\n\nDanh s√°ch m√¥n h·ªçc:\n"
                        for row in mon_list:
                            joined += f"- {row.get('t√™n h·ªçc ph·∫ßn', '')} ({row.get('m√£ h·ªçc ph·∫ßn', '')}), TC: {row.get('s·ªë tc', '')}, TKHP: {row.get('tkhp', '')}, ƒêi·ªÉm ch·ªØ: {row.get('ƒëi·ªÉm ch·ªØ', '')}\n"

                        chunk_texts.append(joined.strip())
                        # metadatas.append({
                        #     "loai": "tong_ket",
                        #     "mssv": mssv,
                        #     "tong_tin_chi": tong_tc,
                        #     "tong_tc_x_diem": round(tong_tc_diem, 2),
                        #     "gpa_toankhoa": gpa
                        # })

                        # T√≠nh GPA t·ª´ng h·ªçc k·ª≥
                        grouped_by_sem = defaultdict(list)
                        for row in rows:
                            hoc_ky = row.get("h·ªçc k·ª≥", "Kh√¥ng r√µ h·ªçc k·ª≥")
                            grouped_by_sem[hoc_ky].append(row)

                        for hoc_ky, sem_rows in grouped_by_sem.items():
                            tong_tc_hk = 0
                            tong_tc_diem_hk = 0
                            mon_list_hk = []

                            for row in sem_rows:
                                diem_chu = row.get("ƒëi·ªÉm ch·ªØ", "").strip().upper()
                                if diem_chu == "F":
                                    continue
                                ma_hp = row.get("m√£ h·ªçc ph·∫ßn", "").upper()
                                ten = row.get("t√™n h·ªçc ph·∫ßn", "").lower()
                                try:
                                    tc = float(row.get("s·ªë tc", "0"))
                                except:
                                    continue
                                if "tƒÉng c∆∞·ªùng" in ten or ma_hp.startswith("TATC"):
                                    continue
                                if tc == 1 and any(kw in ten for kw in ["b√≥ng chuy·ªÅn", "c·∫ßu l√¥ng", "c·ªù vua", "qu·∫ßn v·ª£t"]):
                                    continue
                                diem_thap = {"A": 4.0, "B": 3.0, "C": 2.0, "D": 1.0}.get(diem_chu, 0)
                                tong_tc_hk += tc
                                tong_tc_diem_hk += tc * diem_thap
                                mon_list_hk.append(row)

                            gpa_hk = round(tong_tc_diem_hk / tong_tc_hk, 2) if tong_tc_hk > 0 else 0

                            joined_sem = f"[D·ªØ li·ªáu ƒëi·ªÉm sinh vi√™n]\nM√£ sinh vi√™n: {mssv}\nH·ªçc k·ª≥: {hoc_ky}\nGPA h·ªçc k·ª≥: {gpa_hk}\n\nDanh s√°ch m√¥n h·ªçc:\n"
                            for row in sem_rows:
                                joined_sem += f"- {row.get('t√™n h·ªçc ph·∫ßn', '')} ({row.get('m√£ h·ªçc ph·∫ßn', '')}), TC: {row.get('s·ªë tc', '')}, ƒêi·ªÉm qu√° tr√¨nh: {row.get('qu√° tr√¨nh', '')}, ƒêi·ªÉm thi: {row.get('thi', '/')}, TKHP: {row.get('tkhp', '')}, ƒêi·ªÉm ch·ªØ: {row.get('ƒëi·ªÉm ch·ªØ', '')}\n"

                            chunk_texts.append(joined_sem.strip())
                            # metadatas.append({
                            #     "loai": "diem",
                            #     "mssv": mssv,
                            #     "h·ªçc_k·ª≥": hoc_ky,
                            #     "gpa_hocky": gpa_hk,
                            #     "tong_tc_hk": tong_tc_hk,
                            #     "tong_tc_diem_hk": round(tong_tc_diem_hk, 2)
                            # })

                elif "m√£ h·ªçc ph·∫ßn" in df.columns and "t√™n h·ªçc ph·∫ßn" in df.columns:
                    for _, row in df.iterrows():
                        text_content = f"""[D·ªØ li·ªáu m√¥n h·ªçc]
M√£ h·ªçc ph·∫ßn: {row.get('m√£ h·ªçc ph·∫ßn', '')}
T√™n h·ªçc ph·∫ßn: {row.get('t√™n h·ªçc ph·∫ßn', '')}
S·ªë t√≠n ch·ªâ: {row.get('s·ªë t√≠n ch·ªâ', row.get('s·ªë tc', ''))}
H·ªçc k·ª≥: {row.get('h·ªçc k·ª≥', '')}
Kh·ªëi ki·∫øn th·ª©c: {row.get('kh·ªëi ki·∫øn th·ª©c', '')}
"""
                        chunk_texts.append(text_content.strip())
                        # metadatas.append({
                        #     "loai": "mon_hoc",
                        #     "m√£_m√¥n": row.get("m√£ h·ªçc ph·∫ßn", ""),
                        #     "t√™n_m√¥n": row.get("t√™n h·ªçc ph·∫ßn", ""),
                        #     "s·ªë_t√≠n_ch·ªâ": row.get("s·ªë t√≠n ch·ªâ", row.get("s·ªë tc", "")),
                        #     "h·ªçc_k·ª≥": row.get("h·ªçc k·ª≥", ""),
                        #     "nhom": row.get("kh·ªëi ki·∫øn th·ª©c", "").strip().lower()
                        # })
                else:
                    print(f"‚ö†Ô∏è File {filename} kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng d·ª± ki·∫øn.")

        elif filename.lower().endswith((".docx", ".pdf", ".txt")):
            elements = partition(filename=file_path)
            chunks = chunk_elements(elements, max_characters=512)
            for i, chunk in enumerate(chunks):
                chunk_texts.append(f"[D·ªØ li·ªáu quy ch·∫ø]\n{chunk}")
                # metadatas.append({
                #     "loai": "quy_che",
                #     "file": filename,
                #     "chunk_index": i
                # })

        else:
            print(f"‚ö†Ô∏è Kh√¥ng h·ªó tr·ª£ ƒë·ªãnh d·∫°ng {filename}. B·ªè qua.")
            continue

        if chunk_texts:
            embeddings = model.encode(chunk_texts, batch_size=32, show_progress_bar=True, normalize_embeddings=True)
            collection.add(
                embeddings=embeddings.tolist(),
                documents=chunk_texts,
                # metadatas=metadatas,
                ids=[f"{file_count}_{i}" for i in range(len(chunk_texts))]
            )
            print(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(chunk_texts)} ƒëo·∫°n t·ª´ {filename}")
            file_count += 1
        else:
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y n·ªôi dung h·ª£p l·ªá trong {filename}")

    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω {filename}: {e}")

# ==== T·ªïng k·∫øt ====
print(f"üåü T·ªïng s·ªë ƒëo·∫°n ƒë√£ l∆∞u v√†o Chroma: {collection.count()}")